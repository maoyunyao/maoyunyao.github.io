<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[分支限界法]]></title>
    <url>%2F2019%2F05%2F18%2Falgorithm_branch_bound%2F</url>
    <content type="text"><![CDATA[分支限界法01背包问题]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[利用GPU训练]]></title>
    <url>%2F2019%2F05%2F17%2Fpytorch_train_with_gpu%2F</url>
    <content type="text"><![CDATA[Train with GPU之前我们已经讨论过 Pytorch 对与 CUDA 的良好支持, 那么自然地我们可以使用强悍的 GPU 设备来训练搭建好的神经网络, 以此来达到加速训练的效果. 毕竟炼丹还是需要良好的硬件支持的是吧. 单GPU训练使用单个 GPU 训练的方法非常简单, 我们只需要将网络部署到 CUDA 的 GPU 设备上并且将数据也部署到相同的设备上即可. 部署的方式与我们之前讨论过的 Tensor 的部署方法相同. 123&gt;&gt;&gt; device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")&gt;&gt;&gt; net.to(device)&gt;&gt;&gt; inputs, labels = inputs.to(device), labels.to(device) 注意此处 tensor.to(device) 仅仅将原 Tensor 在 GPU 上作一个拷贝并返回拷贝的 Tensor, 并不对原 Tensor 有任何修改. 多GPU训练默认情况下 Pytorch 只会使用单个 GPU 进行模型的训练, 你可以使用 DataParallel 来实现多 GPU 的模型计算, 其原型为 1torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0) 参数解释如下: module: 需要多 GPU 并行计算的模型 device_ids: CUDA 设备的 ID, 默认为全部可用设备 output_device: 输出所在设备, 默认为 device_ids[0] 12345if torch.cuda.device_count() == 3: print("Let's use", torch.cuda.device_count(), "GPUs!") model = nn.DataParallel(model, device_ids=[0, 1, 2])model.to(device) 没错就是这么简单, 结束了. 参考文献: [1]Pytorch - Docs [2]Pytorch - Tutorials: A 60 Minute Blitz]]></content>
      <categories>
        <category>Pytorch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[神经网络搭建]]></title>
    <url>%2F2019%2F05%2F17%2Fpytorch_nn%2F</url>
    <content type="text"><![CDATA[torch.nn我们可以利用 torch.nn 来搭建神经网络, 其中 nn.Mudule 你可以定义一系列网络层以及一个 forward(input) 方法从输入获取神经网络的输出. 神经网络训练流程 定义带有可训练参数 (权重) 的神经网络结构 训练数据输入神经网络前向传播获得输出 利用数据标签和网络输出计算损失函数 执行反向传播算法, 获得损失函数的输出相对于各参数的梯度值 利用梯度值更新网络的权值 定义网络在模型中只需到定义 forward() 方法, 而 backward() 方法会自动定义. 在 forward() 方法中可以使用任何的对 Tensor 的操作. 下面的代码是 LeNet 的 Pytorch 实现: 1234567891011121314151617181920212223242526272829303132333435import torchimport torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): super(Net, self).__init__() # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square you can only specify a single number x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_features 123456789&gt;&gt;&gt; net = Net()&gt;&gt;&gt; print(net)Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True)) 模型中可以学习的参数可以由 net.parameters() 返回, 参数是一个个的 Tensor, requires_grad 标志位为 True. 12345&gt;&gt;&gt; params = list(net.parameters())&gt;&gt;&gt; print(len(params))10&gt;&gt;&gt; print(params[0].size()) # conv1's .weighttorch.Size([6, 1, 5, 5]) 前向传播我们可以尝试给如上的网络一个随机的输入, 观察他的输出情况. 注意此处的输入 size 为 (1, 1, 32, 32), 这是因为 torch.nn 只支持 mini-batches 的输入. 例如, 对于 nn.Conv2d , 它所要求的输入是一个 4 维的 Tensor, 即: nSamples * nChannels * Height * Width. 故而如果你的输入是单个样本, 可以利用 input.unsqueeze(0) 来添加一个虚拟维度. 12345&gt;&gt;&gt; input = torch.randn(1, 1, 32, 32)&gt;&gt;&gt; out = net(input)&gt;&gt;&gt; print(out)tensor([[-0.1258, -0.0585, -0.0102, -0.0407, -0.0695, -0.0808, 0.0731, -0.1482, 0.0575, -0.0164]], grad_fn=&lt;AddmmBackward&gt;) 损失函数损失函数使用 (output, target) 对作为输出, 计算出的值用于度量 output 相对于 target 的距离. porch.nn 中定义了许多现成的损失函数. 一个最简单的例子便是均方误差函数 nn.MSELoss . 123456output = net(input)target = torch.randn(10) # a dummy target, for exampletarget = target.view(1, -1) # make it the same shape as outputcriterion = nn.MSELoss()loss = criterion(output, target) 12&gt;&gt;&gt; print(loss)tensor(0.5244, grad_fn=&lt;MseLossBackward&gt;) 反向传播由于多次反向传播的梯度值的累加机制, 我们在进行一次反向传播更新权重之前, 我们需要用 zero_grad() 对所有参数的 grad 清零, 然后对 loss 调用 backward() 方法就可以得到当前损失函数的输出关于各可训练参数的梯度了. 12345678&gt;&gt;&gt; net.zero_grad()&gt;&gt;&gt; print(net.conv1.bias.grad)tensor([0., 0., 0., 0., 0., 0.])&gt;&gt;&gt; loss.backward()&gt;&gt;&gt; print(net.conv1.bias.grad)tensor([ 0.0185, 0.0110, 0.0038, 0.0098, -0.0105, -0.0002]) 更新网络权重一个最简单的权重更新方法是随机梯度下降(SGD), 他的策略是 weight = weight - learning_rate * gradient , 我们可以简单实现如下: 123learning_rate = 0.01for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) 不过, 作为一个成熟的深度学习框架, Pytorch 应该学会自己更新权重, 故而 torch.optim 实现了许多权重更新的算法供我们使用, 而且使用方法非常简单: 12345678910111213import torch.optim as optim# create your optimizeroptimizer = optim.SGD(net.parameters(), lr=0.01)# in your training loop:optimizer.zero_grad() # zero the gradient buffersoutput = net(input)loss = criterion(output, target)loss.backward()optimizer.step() # Does the update 参考文献: [1]Pytorch - Docs [2]Pytorch - Tutorials: A 60 Minute Blitz]]></content>
      <categories>
        <category>Pytorch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[张量的创建]]></title>
    <url>%2F2019%2F05%2F15%2Fpytorch_tensor_create%2F</url>
    <content type="text"><![CDATA[torch.TensorTensor 是 Pytorch 的数据结构, 非常类似于 Numpy 里的 ndarray. 是一种包含单一数据类型的多维张量. 其上定义了诸多的操作. 如我们讨论过的自动求导机制等. 张量的创建torch.tensor() 1torch.tensor(data,dtype=None,device=None,requires_grad=False,pin_memory=False) 利用上述函数创建 Tensor 时, 它总是拷贝 data 内的数据然后利用这些数据结合相应参数构造一个新的叶结点变量, 故而 torch.tensor(x) 与 x.clone().detach() 等价, 而 torch.tensor(x,requires_grad=True) 则与 x.clone().detach().rquires_grad_(True) 等价. 如果想要避免上述的拷贝-创建机制, 请不要使用这个函数 :-). 而如果你想得到一个与原变量共享内存的新 Tensor, 那么可以使用 torch.Tensor.detach() 或者 torch.as_tensor() 等方法. 前者可以从原来的 Tensor 创造一个 requires_grad 标志位为 False 且不在原 Tensor 计算图中的 Tensor, 两者共享内存. 而后者可从 Numpy 的一个 ndarray 创造 Tensor , 两者共享内存. 如果你想修改原 Tensor 的性质, 可以使用 torch.Tensor.requires_grad_() 或者 torch.Tensor.detach_(). 前者使 Tensor 的 requires_grad 标志位为真, 自动求导机制开始记录操作.后者与其不含下划线的版本不同之处在于它修改原 Tensor 的性质, 即将原 Tensor 从计算图中剥离并且置 requires_grad 为 False. 示例代码: 12345&gt;&gt;&gt; import torch&gt;&gt;&gt; a = [1., 1.]&gt;&gt;&gt; b = [2., 2.]&gt;&gt;&gt; x = torch.tensor(a, requires_grad = True)&gt;&gt;&gt; y = torch.tensor(a, requires_grad = False) 123456&gt;&gt;&gt; x[0] = -1&gt;&gt;&gt; xtensor([-1., 1.], grad_fn=&lt;CopySlices&gt;)&gt;&gt;&gt; a[1.0, 1.0]# 可见 x 与 a 并不共享内存 1234567891011&gt;&gt;&gt; x_ = x.detach()&gt;&gt;&gt; xtensor([-1., 1.], grad_fn=&lt;CopySlices&gt;)&gt;&gt;&gt; x_tensor([-1., 1.])&gt;&gt;&gt; x[0] = 1&gt;&gt;&gt; xtensor([1., 1.], grad_fn=&lt;CopySlices&gt;)&gt;&gt;&gt; x_tensor([1., 1.])# 可见 x 与 x_ 共享内存 torch.as_tensor() 1torch.as_tensor(data, dtype=None, device=None) 如果 data 是一个 Tensor 且数据类型与设备参数相同, 则直接共享该 Tensor 的内存给新 Tensor. 如果 data 是一个 Tensor 而数据类型或设备不符, 则会新创建一个 Tensor, 新 Tensor 保持原 Tensor 的计算图和 requires_grad 标志位. 如果 data 是一个数据类型相同的 ndarray , 且参数中指定的设备为 CPU, 则新 Tensor 与该 ndarray 共享内存. 示例代码: 1234567891011&gt;&gt;&gt; a = numpy.array([1, 2, 3])&gt;&gt;&gt; t = torch.as_tensor(a)&gt;&gt;&gt; t[0] = -1&gt;&gt;&gt; aarray([-1, 2, 3])&gt;&gt;&gt; a = numpy.array([1, 2, 3])&gt;&gt;&gt; t = torch.as_tensor(a, device=torch.device('cuda'))&gt;&gt;&gt; t[0] = -1&gt;&gt;&gt; aarray([1, 2, 3]) 其他一些创建方法123456789101112131415161718192021&gt;&gt;&gt; torch.ones(5, 6)tensor([[1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.]])&gt;&gt;&gt; q = torch.zeros(5, 6)tensor([[0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.]])&gt;&gt;&gt; torch.ones_like(q)tensor([[1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.]])...... 参考文献: [1]Pytorch - Docs [2]Pytorch - Tutorials: A 60 Minute Blitz]]></content>
      <categories>
        <category>Pytorch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[自动求导机制]]></title>
    <url>%2F2019%2F05%2F14%2Fpytorch_autograd%2F</url>
    <content type="text"><![CDATA[自动求导requires_grad 标志位Pytorch 允许我们利用每个 Tensor 的 requires_grad 标志位来在计算图中排除一些不需要进行梯度计算的子图. 一个操作的输入中只要有一个Tensor的 requires_grad 为True, 则该操作的输出的 requires_grad 为True.相反只有输入中所有的Tensor的 requires_grad 为False时该操作的输出的 requires_grad 才为False. 123456789&gt;&gt;&gt; x = torch.randn(5, 5) # requires_grad = False by default&gt;&gt;&gt; y = torch.randn(5, 5) # requires_grad = False by default&gt;&gt;&gt; z = torch.randn((5, 5), requires_grad = True)&gt;&gt;&gt; a = x + y&gt;&gt;&gt; a.requires_gradFalse&gt;&gt;&gt; b = a + z&gt;&gt;&gt; b.requires_gradTrue 在 model 中我们可以通过如下的操作改变其参数的 requires_grad 标志位以实现特定的需求, 如通过设置标志位为False来进行参数微调、跑测试集等. 123model = torchvision.models.resnet18(pretrained = True)for param in model.parameters(): param.requires_grad = False; 当然我们也可以用如下的方法禁用梯度计算 123456&gt;&gt;&gt; import torch&gt;&gt;&gt; x = torch.randn((5, 5), requires_grad = True)&gt;&gt;&gt; with torch.no_grad():... y = x + 1&gt;&gt;&gt; y.requires_gradFalse 反向传播求导如果你想要求输出 Tensor 关于各参数的偏导数, 可对输出的 Tensor 调用 backward() 方法 . 如果输出的 Tensor 是一个标量, 则无需对 backward() 指定任何参数. 而如果输出的 Tensor 不是一个标量, 则需为该方法指定一个参数, 该参数为一个 Tensor, 其形状与输出的 Tensor 相同, 其值作为计算梯度时输出 Tensor 各元素乘上的系数. 输出为标量的例子: 1234567891011&gt;&gt;&gt; x = torch.ones(2, 2, requires_grad = True)&gt;&gt;&gt; y = x + 2&gt;&gt;&gt; z = y * y * 3&gt;&gt;&gt; out = z.mean()&gt;&gt;&gt; outtensor(27., grad_fn=&lt;MeanBackward0&gt;)&gt;&gt;&gt; out.backward()&gt;&gt;&gt; x.grad #输出为out对x各元素的偏导数tensor([[4.5000, 4.5000], [4.5000, 4.5000]]) 输出为向量的例子: 1234567&gt;&gt;&gt; x = torch.tensor([[1., 1.], [2., 2.]], requires_grad = True)&gt;&gt;&gt; y = torch.tensor([[3., 3.], [4., 4.]], requires_grad = True) &gt;&gt;&gt; z = x.mm(y)&gt;&gt;&gt; z.backward(torch.ones(2, 2))&gt;&gt;&gt; x.gradtensor([[6., 8.], [6., 8.]]) 可以发现当输出不是标量时, 如果传给 backward() 的参数为一个全一的 Tensor (正如我们上面所做), 则反向传播求得的对输入的梯度值是输出 Tensor 中各个元素对输入的梯度的累加值. 若想单独得到输出 Tensor 的某一个元素对输入的梯度值, 则可以通过给 backward() 提供特定的参数实现, 比如下面的例子就展示了矩阵乘法情况下输出Tensor z 的左上角元素对输入的梯度求解过程. 12345678&gt;&gt;&gt; x = torch.tensor([[1., 1.], [2., 2.]], requires_grad = True)&gt;&gt;&gt; y = torch.tensor([[3., 3.], [4., 4.]], requires_grad = True) &gt;&gt;&gt; z = x.mm(y)&gt;&gt;&gt; a = torch.tensor([[1., 0.], [0., 0.]])&gt;&gt;&gt; z.backward(a)&gt;&gt;&gt; x.gradtensor([[3., 4.], [0., 0.]]) 如果调用 backward() 方法时没有设置参数 retain_graph = True, 则在完成一次 backward() 后计算图会被释放, 无法进行第二次反向传播. 设置 retain_graph = True 的作用是保持计算图不释放, 每次反向传播的梯度累加. 而若需要将 Tensor 的梯度清零, 则可调用 zero_() 函数: 123456789&gt;&gt;&gt; x.gradtensor([[6., 8.], [6., 8.]])&gt;&gt;&gt; x.grad.zero_()tensor([[0., 0.], [0., 0.]])&gt;&gt;&gt; x.gradtensor([[0., 0.], [0., 0.]]) 参考文献: [1]Pytorch - Docs [2]Pytorch - Tutorials: A 60 Minute Blitz]]></content>
      <categories>
        <category>Pytorch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CUDA支持]]></title>
    <url>%2F2019%2F05%2F14%2Fpytorch_cuda%2F</url>
    <content type="text"><![CDATA[torch.cudaPytorch 提供了 CUDA 的支持, torch.cuda 模块用于设置和运行 CUDA 上的操作, 它会记住所指定的 GPU, 你后续所新建的 CUDA 上的 Tensor 都将被默认在该 GPU 上创建. 当然你也可以用torch.cuda.device修改 CUDA 默认指定的 GPU 或者为 Tensor 单独指定 GPU. 模块不支持跨 GPU 的操作, 除非使用类似拷贝的函数功能将变量转换到同一设备上, 如to()和cuda()等． 为cuda上的Tensor指定设备 为每个Tensor单独指定/转换到一个设备 123456789101112131415161718&gt;&gt;&gt; import torch&gt;&gt;&gt; cuda = torch.device('cuda') # 使用cuda默认的设备GPU0&gt;&gt;&gt; cuda1 = torch.device('cuda:1') # 指定GPU1&gt;&gt;&gt; cuda2 = torch.device('cuda:2') # 指定GPU2&gt;&gt;&gt; x = torch.tensor([1., 2.], device = cuda)&gt;&gt;&gt; x.devicedevice(type='cuda', index=0)&gt;&gt;&gt; y = torch.tensor([1., 2.])&gt;&gt;&gt; y.devicedevice(type='cpu')&gt;&gt;&gt; y = y.cuda(cuda1)&gt;&gt;&gt; y.devicedevice(type='cuda', index=1)&gt;&gt;&gt; z = torch.tensor([1., 2.])device(type='cpu')&gt;&gt;&gt; z = z.to(cuda2)&gt;&gt;&gt; z.devicedevice(type='cuda', index=2) 设置上下文默认设备 123456789101112131415161718# 指定cuda的GPU为GPU1&gt;&gt;&gt; with torch.cuda.device(1):... a = torch.tensor([1., 2.], device = cuda)... b = torch.tensor([1., 2.]).cuda()... c = torch.tensor([1., 2.]).to(device = cuda)... d = a + b + c... # 指定上下文后还是可以为每个tensor单独指定设备... s = torch.tensor([1., 2.]).to(device = cuda2)&gt;&gt;&gt; a.devicedevice(type='cuda', index=1)&gt;&gt;&gt; b.devicedevice(type='cuda', index=1)&gt;&gt;&gt; c.devicedevice(type='cuda', index=1)&gt;&gt;&gt; d.devicedevice(type='cuda', index=1)&gt;&gt;&gt; s.devicedevice(type='cuda', index=2) 1234print("Outside device is 0") # On device 0 (default in most scenarios)with torch.cuda.device(1): print("Inside device is 1") # On device 1print("Outside device is still 0") # On device 0 Pytorch中CUDA使用的一个范例由参数和主机的 CUDA 是否可用共同决定是使用 CPU 还是 CUDA 123456789101112import argparseimport torchparser = argparse.ArgumentParser(description='PyTorch Example')parser.add_argument('--disable-cuda', action='store_true', help='Disable CUDA')args = parser.parse_args()args.device = Noneif not args.disable_cuda and torch.cuda.is_available(): args.device = torch.device('cuda')else: args.device = torch.device('cpu') 有了制定了设备后就可以进行 Tensor 或者 model 的部署了 12x = torch.empty((8, 42), device=args.device)net = Network().to(device=args.device) 参考文献: [1]Pytorch - Docs [2]Pytorch - Tutorials: A 60 Minute Blitz]]></content>
      <categories>
        <category>Pytorch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[似然函数作为损失函数]]></title>
    <url>%2F2019%2F05%2F13%2Fml_loss_function%2F</url>
    <content type="text"><![CDATA[似然函数在数理统计学中, 似然函数是一种关于统计模型中的参数的函数, 表示模型参数的似然性. 统计学中似然性和概率有着明确的区分. 概率用于在已知一些参数的情况下, 预测接下来在观测上所得到的结果. 而似然性则是用于在已知某些观测所得到的结果时, 对相关参数进行估值. 似然函数的重要性不是它的具体取值, 而是当参数变化时函数到底是变大还是变小. 对同一个似然函数, 其所代表的模型中, 某项参数值具有多种可能, 但如果存在一个参数值使得它的函数值达到最大的话, 那么这个值就是该项参数最为”合理”的参数值. 在已知某个参数 $B$ 时, 事件 $A$ 会发生的概率写作: P(A|B) = \frac{P(A,B)}{P(B)}由贝叶斯公式, 可知: P(B|A) = \frac{P(A|B)P(B)}{P(A)}故而, 我们可以反过来构造表示似然性的方法, 即已知有事件 $A$ 发生, 运用似然函数 $L(B|A)$ 估计参数 $B$ 的可能性.前面我们已经提到, 似然函数的重要性在于相对大小而不要求满足归一化条件. 因此我们将一个似然函数乘以一个正的常数后仍然是似然函数. 而对于特定的问题, $\frac{P(B)}{P(A)}$ 为不变的常数, 则: L(b|A) = \alpha P(A|B=b)当令 $\alpha=1$ 时, 关于参数 $B$ 的似然函数与给定参数 $B$ 的值后观测 $A$ 的条件概率等价. 似然函数作为损失函数既然似然函数可以用于在已知观测结果时对模型参数进行估值, 那么自然地可以作为机器学习中的损失函数来指导训练过程. 考虑判别式模型中的场景, 作为损失函数的似然函数接收 $\textbf{label}$ 与 $\textbf{predict}$ 两部分输入. 其中 $\textbf{label}$ 为训练集的标签也就是一次观测时各类别的确切概率值, 如 $[0,…0,1,0…0]$. 而 $\textbf{predict}$ 为模型在给定输入后输出的各类别预测概率分布. 似然函数的目的就是衡量 $\textbf{predict}$ 背后的模型对于观测值,即 $\textbf{label}​$ 的解释程度. 那么此时结合之前似然函数的讨论, 我们需要最大化似然函数, 即使得基于预测 $\textbf{predict}$ 时产生观测 $\textbf{label}$ 的概率最大. 单个样本时, 我们计算给定 $\textbf{predict}$ 时产生观测 $\textbf{label}$ 的概率如下: P = \prod_{i=1}^{C}predict(i)^{label(i)}\\ 注:其中C为类别数目下面我们对整个样本集(符合独立同分布定理)进行计算分析, 可将似然函数等价为条件概率: L = \prod_{j=1}^{m}\prod_{i=1}^{C}predict(i)^{label(i)}\\ 注:其中m为训练集大小由于指数以及累乘的存在, 很自然的我们想到对似然函数进行对数运算. 于是得到了所谓的对数似然函数. 由于对数的单调特性, 最大化似然函数等价于最大化对数似然函数. L = \sum\limits_{j=1}^{m}\sum\limits_{i=1}^{C}label(i)*log(predict(i))\\对数似然函数与交叉熵对数似然函数的形式与交叉熵极其相似, 事实上两者只相差了常系数项. 将上式中的对数似然函数除以m并取负数便得到所谓的负对数似然函数, 与交叉熵的形式一模一样. 于是原优化问题转化为最小化负对数似然函数, 即交叉熵. 参考文献: [1]维基百科-似然函数 [2]机器之心-似然函数与交叉熵]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习之核函数]]></title>
    <url>%2F2019%2F05%2F12%2Fml_kernel_method%2F</url>
    <content type="text"><![CDATA[核函数对于线性不可分样本问题(如简单的异或问题), 可以将样本从原始空间映射到一个更高维的特征空间, 使得样本在这个特征空间内线性可分. 例如在支持向量机中, 原问题转化为求解特征空间上的凸二次规划问题: f(\textbf{x}) = w^T \phi(\textbf{x}) + b对上述线性可分支持向量机的求解涉及到如下对偶问题的求解(由拉格朗日乘子法得到对偶问题的具体过程此处不予讨论): \mathop{max}_\alpha \quad \sum\limits_{i = 1}^{n}\alpha_i-\frac{1}{2}\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{m}\alpha_i\alpha_jy_iy_j\phi(\textbf{x}_i)^T\phi(\textbf{x}_j)\\ s.t. \quad \sum\limits_{i=1}^{m}\alpha_iy_i=0\\ \alpha_i \ge 0, \quad i=1,2,...,m求解上式涉及到计算 $\phi(\textbf{x}_i)^T\phi(\textbf{x}_j)$, 这是样本 $\textbf{x}_i$ 与$\textbf{} $$\textbf{x}_j$ 映射到特征空间之后的内积. 由于特征空间维数很高, 甚至可能是无穷维, 因此直接计算通常是困难的. 为了避开这个障碍, 可以设想这样一个函数: k(\textbf{x}_i,\textbf{x}_j)==\phi(\textbf{x}_i)^T\phi(\textbf{x}_j)即 $\textbf{x}_i$ 与 $\textbf{x}_j$ 在特征空间的内积等于它们在原始样本空间中通过函数 $k(.,.)$ 计算的结果(此即核技巧). 有了这样的函数我们就不必直接去计算高维甚至无穷维特征空间中的内积, 于是对偶问题重写并求解可得 f(\textbf{x})=w^T\phi(\textbf{x})+b\\ =\sum\limits_{i=1}^{m}\alpha_iy_i\phi(\textbf{x}_i)^T\phi(\textbf{x})+b\\ =\sum\limits_{i=1}^{m}\alpha_iy_ik(\textbf{x},\textbf{x}_i)+b\\这里的函数 $k(.,.)$ 就是 “核函数” 显然, 如果已知合适的映射 $\phi(.)$ 的具体形式, 则可写出核函数 $k(.,.)$. 但是 现实任务重我们通常不知道 $\phi(.)$ 是什么形式. 剩下的问题就是如何选择 $\phi(.)$: 使用一个通用的 $\phi$, 例如无限维的 $\phi$, 它隐含地用在高斯核也即基于径向基函数 $(RBF)$ 核的核机器上 手动地设计 $\phi$ , 在深度学习出现以前, 这一直是主流的方法 深度学习的策略是去学习 $\phi$ 参考文献: [1]机器学习-周志华-第126, 127页 [2]深度学习-花书-第106页]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Inline Assembler in GCC]]></title>
    <url>%2F2018%2F10%2F05%2Finline-asm-gcc%2F</url>
    <content type="text"><![CDATA[操作系统上机知识储备(一)对gcc所支持的内联汇编格式及使用方法进行简要的介绍 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117/* * This c file is a introduction of inline Assembly for gcc * Time： 2018/9/8 * Author: myy * Reference： JeffreyLi https://www.jianshu.com/p/1782e14a0766*//////////////////////////////////////////////////////////////////////////////////////////////*Formular: asm ( assembler template : output operands // optional : input operands // optional : list of clobbered registers // optional ); The assembler template form: * form1:"sentence?sentence?sentence" * form2: "sentence?" "sentence?" "sentence" --- Where the ? should be replaced by "\n\t" or ";". --- Access to C variables by %0 %1 and so on. The operands form: * "constraint" (C expression) --- Where constraint is mainly used for note Addressing type(memory or register). --- For output operants, it should be decorated by "=", which means write-only. --- For multiple operants, they should be seperated by ",". --- To use the operants in assempler template with %0 %1 %2 ... - First output operants then input operants. - Signed in the list order. --- The output operants must be lvalue. The clobber list: * When a register is used in the assembler template, it should be in the clobber list. Which let gcc no longer assume that the values previously stored in these registers are still legal. * If the instruction changes the memory value in an unpredictable form, you need to add "memory" to the clobbered list.This allows GCC not to cache these memory values. The volatile: * If the assembly code is required to be executed at the location where it was placed (for example, it cannot be moved out of the loop during the optimization), we need to put a "volatile" keyword before the "()" after asm. This will prevent these codes from being moved or deleted. --- Useage: __asm__ __volatile__(...); Common constraints: *********************** * r Register(s) * a %eax, %ax, %al * b %ebx, %bx, %bl * c %ecx, %cx, %cl * d %edx, %dx, %adl * S %esi, %si * D %edi, %di *********************** * m memory *********************** --- If use the register constraint, the value will first be stored at the register used, and then write back to the memory in the end. --- Constraint "0" means: use the same register as the first output operants. Constraint Modifiers: * "=": Write only. * "&amp;": Means that this operand is an earlyclobber operand, which is modified before the instruction is finished using the input operands. Therefore, this operand may not lie in a register that is used as an input operand or as a part of any memory address.An input operand can be tied to an earlyclobber operand if its only use as an input occurs before the early result is written.*/////////////////////////////////////////////////////////////////////////////////////////////#include&lt;stdio.h&gt;int main()&#123; int a = 10,b = 5,c; //Example: to realize c = a + b __asm__ __volatile__ ( "movl %2,%%eax\n\t movl %1,%%ebx\n\t addl %%ebx,%%eax\n\t movl %%eax,%0" :"=r"(c) :"r"(a),"r"(b) :"eax","ebx" ); printf("c = a + b = %d\n",c); return 0;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[FFT with python]]></title>
    <url>%2F2018%2F04%2F01%2Ffft-by-python%2F</url>
    <content type="text"><![CDATA[来自信号与系统的一个小作业用python和其开源的科学计算库numpy实现快速傅里叶变换算法，并用数据可视化开源第三方库matplotlib作出频域图样。 代码实现如下12345678910111213141516171819202122232425262728293031323334#导入第三方库###############################from numpy import *from math import *import matplotlib.pyplot as plt################################ TO GET THE DATA（take gauss signal sampling for example）data_list = [ exp(-pi*x*x/1024/1024) for x in range(0,1024)]# FFT FUNCTION PART (递归蝶形算法，输入为数据点数目和数据点列)def my_fft_fun(NUM, DATA): if NUM == 1: return DATA else: ODD_DATA=DATA[0::2] EVEN_DATA=DATA[1::2] ODD_LIST=my_fft_fun(NUM//2, ODD_DATA) #奇偶分治 EVEN_LIST=my_fft_fun(NUM//2, EVEN_DATA) #奇偶分治 RES_DATA=[ EVEN_LIST[k%(NUM//2)]+ODD_LIST[k%(NUM//2)]*(cos(2*pi*k/NUM)+sin(2*pi*k/NUM)*1j) for k in range(0,NUM)] return RES_DATA#调用定义好的FFT递归求解函数FINAL_RES=my_fft_fun(1024,data_list)# 作出X(k)的模值与k的关系图x_part=range(0,1024)y_part=[ abs(val) for val in FINAL_RES]plt.scatter(x_part,y_part,s=5)plt.title('DFT for Gauss Signal', fontsize=20)plt.xlabel('k', fontsize=14)plt.ylabel('X(k)', fontsize=14)plt.show() 效果作图如下 有待改进之处算法仅涉及简单情形，即数据点的个数是2的指数倍，当需要计算非2的指数倍个数据点的FFT时，分治算法需要更加复杂。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Zero contact]]></title>
    <url>%2F2018%2F02%2F08%2Fzero-contact%2F</url>
    <content type="text"><![CDATA[不如来段紧张刺激的Hello,World代码如何： 1&gt;&gt;&gt; print("Hello,World!") THE END…]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F01%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
