<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Attention Mechanisms]]></title>
    <url>%2F2019%2F08%2F08%2Fml_attention%2F</url>
    <content type="text"><![CDATA[Attention MechanismsIn recent years, attention model has been broadly applied to natural language processing, image classification, speech Recognition and other different kinds of deep learning tasks. Human’s attentionFrom the naming of attention model, it is obvious that it draws on the attention mechanism of human beings. The visual attention mechanism is a brain signal processing mechanism unique to human vision. By quickly scanning the global image, human vision obtains the target area that needs to be focused on, which is the focus of attention, and then invests more attention resources in this area to obtain more detailed information about the target. Other useless information are thus suppressed. Encoder-Decoder Framework Source = \\ Target = \\ Encoder: \textbf{C} = \textbf{F}(\textbf{x}_1,\textbf{x}_2,...,\textbf{x}_m)Decoder: \textbf{y}_i = \textbf{g}(\textbf{C},\textbf{y}_1,\textbf{y}_2,...,\textbf{y}_{i-1})Attention ModelLet’s take machine translation as an example to explain the basic principles of the most common Soft Attention model. After that we leave the Encoder-Decoder framework to abstract the essence of the attention mechanism. Finally, we give a brief introduction of the recently widely used Self Attention model. Soft Attention:When applying Encoder-Decoder framework to machine translation task without the attention model. It’s easy to discover that all words in the target are generated from the same semantic representation $\textbf{C}$. \begin{array}{l}{\mathbf{y}_{1}=\mathbf{f}(\mathbf{C})} \\ {\mathbf{y}_{2}=\mathbf{f}\left(\mathbf{C}, \mathbf{y}_{1}\right)} \\ {\mathbf{y}_{3}=\mathbf{f}\left(\mathbf{C}, \mathbf{y}_{1}, \mathbf{y}_{2}\right)}\end{array}\\ ...If the $Source$ sentence is long, all the semantics are completely represented by an intermediate semantic vector $\textbf{C}$, the information of the word itself will disappear. But if we introduce attention mechanism into the translation task, the original intermediate semantic representation $\textbf{C}$ will be replaced with $\textbf{C}_i$ when generating the word $\textbf{y}_i$. \begin{array}{l}{\mathbf{y}_{1}=\mathbf{f}(\mathbf{C_1})} \\ {\mathbf{y}_{2}=\mathbf{f}\left(\mathbf{C_2}, \mathbf{y}_{1}\right)} \\ {\mathbf{y}_{3}=\mathbf{f}\left(\mathbf{C_3}, \mathbf{y}_{1}, \mathbf{y}_{2}\right)}\end{array}\\ ... Every $\textbf{C}_i​$ may corresponding to different attention distribution probability of the source words: \textbf{C}_{i}=\sum_{j=1}^{\mathbf{L}_{x}} \textbf{a}_{i j} \textbf{h}_{j}Where $L_x$ represents the length of Source sentence, $a_{ij}$ represents the attention distribution probability of the word $\textbf{x}_j$ in Source sentence with respect to word $\textbf{y}_i$ in Target sentence. And $h_j$ is the semantic So, here comes the problem. How do we get the attention distribution probability of the Source sentence for every Target word? Keep updating… Reference:[1] https://blog.csdn.net/tg229dvt5i93mxaq5a6u/article/details/78422216 [2] https://blog.csdn.net/malefactor/article/details/50550211 [3] https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MOT]]></title>
    <url>%2F2019%2F08%2F06%2Fpaper_MOT_survey%2F</url>
    <content type="text"><![CDATA[Multiple Object TrackingAn investigation of real time multiple object tracking. Mainly focused on Tracking-by-detection paradigm IntroductionMost recent approaches follow the popular “tracking-by-detection” paradigm. The overall steps are roughly as follows: Objects to be tracked are firstly detected with bounding boxes in each frame. When a new frame come with its detected bounding boxes, we calculate similarities between newly detected objects and existing tracklets before this frame. Associating the newly detected objects with existing tracklets based on the similarities we calculated in step 2. Do some dirty details like tracklets deletion/creation etc. Similarity MetricsA robust similarity score is key to the success of trackers. And a good similarity score is expected to reflect multiple cues. The commonly considered similarity metrics are as follows: Appearance Location/Motion Topological Association algorithmThe most commonly used data association algorithm in MOT problem is the Hungarian algorithm. A brief introduction can be found here. Denote the $i^{th}$ tracklet before frame $t-1$ as $\textbf{T}_i^{t-1}=\{\textbf{b}_i^1,\textbf{b}_i^2,…,\textbf{b}_i^{t-1}\}$ and the detected objects at current frame $t$ as $\textbf{D}_t = \{\textbf{b}_j^t\}_{j=1}^{N_t}$. Each pair $(\textbf{T}_i^{t-1},\textbf{b}_j^t)$ is assigned a similarity score $s_{ij}^t$. Considering all existing tracklets before frame $t-1$, we can get a bipartite graph. Now the Hungarian algorithm can be adopted on the bipartite graph to get the optimal assignments. Related paperKeep Updating…..]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hungarian Algorithm]]></title>
    <url>%2F2019%2F08%2F04%2Falgorithm_hungarian%2F</url>
    <content type="text"><![CDATA[Hungarian AlgorithmBrief introduction:A combinatorial optimization algorithm that solves the assignment problem in polynomial time. It was developed and published in 1955 by Harold Kuhn, who gave the name “Hungarian method” because the algorithm was largely based on the earlier works of two Hungarian mathematicians: Dénes Kőnig and Jenő Egerváry. There are two ways to formulate the problem: as a matrix or as a bipartite graph. Matrix Formulation:In the matrix formulation, we are given a nonnegative n×n matrix, where the element in the i-th row and j-th column represents the cost of assigning the j-th job to the i-th worker. We have to find an assignment of the jobs to the workers, such that each job is assigned to one worker and each worker is assigned one job, such that the total cost of assignment is minimum. Bigraph Formulation:In the matrix formulation, we have a complete bipartite graph $G=(S,T;E)​$ with n worker vertices ($S​$) and n job vertices ($T​$), and each edge has a nonnegative cost $c(i,j)​$. We want to find a perfect matching with a minimum total cost. Algorithm:Here we describe the algorithm in terms of matrix graphs. Theorem: If a number is added to or subtracted from all of the entries of any one row or column of a cost matrix, then on optimal assignment for the resulting cost matrix is also an optimal assignment for the original cost matrix. The following algorithm applies the above theorem to a given n × n cost matrix to find an optimal assignment: Step 1. Subtract the smallest entry in each row from all the entries of its row. Step 2. Subtract the smallest entry in each column from all the entries of its column. Step 3. Draw lines through appropriate rows and columns so that all the zero entries of the cost matrix are covered and the minimum number of such lines is used. Step 4. Test for Optimality: If the minimum number of covering lines is n, an optimal assignment of zeros is possible and we are finished. If the minimum number of covering lines is less than n, an optimal assignment of zeros is not yet possible. In that case, proceed to Step 5. Step 5. Determine the smallest entry not covered by any line. Subtract this entry from each uncovered row, and then add it to each covered column. Return to Step 3. An example:Problem: \left[ \begin{matrix} 90 & 75 & 75 & 80\\ 35 & 85 & 55 & 65 \\ 125 & 95 & 90 & 105 \\ 45 & 110 & 95 & 115 \end{matrix} \right]Step 1: Subtract 75 from Row 1, 35 from Row 2, 90 from Row 3, and 45 from Row 4. \left[ \begin{matrix} 90 & 75 & 75 & 80\\ 35 & 85 & 55 & 65 \\ 125 & 95 & 90 & 105 \\ 45 & 110 & 95 & 115 \end{matrix} \right] \to \left[ \begin{matrix} 15 & 0 & 0 & 5 \\ 0 & 50 & 20 & 30 \\ 35 & 5 & 0 & 15 \\ 0 & 65 & 50 & 70 \\ \end{matrix} \right]Step 2: Subtract 0 from Column 1, 0 from Column 2, 0 from Column 3, and 5 from Column 4. \left[ \begin{matrix} 15 & 0 & 0 & 5 \\ 0 & 50 & 20 & 30 \\ 35 & 5 & 0 & 15 \\ 0 & 65 & 50 & 70 \\ \end{matrix} \right] \to \left[ \begin{matrix} 15 & 0 & 0 & 0 \\ 0 & 50 & 20 & 25 \\ 35 & 5 & 0 & 10 \\ 0 & 65 & 50 & 65 \\ \end{matrix} \right]Step 3: Cover all the zeros of the matrix with the minimum number of horizontal or vertical lines. \left[ \begin{matrix} 15 & 0 & 0 & 0 \\ 0 & 50 & 20 & 25 \\ 35 & 5 & 0 & 10 \\ 0 & 65 & 50 & 65 \\ \end{matrix} \right] \to \left[ \begin{matrix} \color{red}{15} & \color{red}0 & \color{red}0 & \color{red}0 \\ \color{red}0 & 50 & \color{red}{20} & 25 \\ \color{red}{35} & 5 & \color{red}0 & 10 \\ \color{red}0 & 65 & \color{red}{50} & 65 \\ \end{matrix} \right]Step 4: Since the minimal number of lines is less than 4, we have to proceed to Step 5. Step 5: Note that 5 is the smallest entry not covered by any line. Subtract 5 from each uncovered row. \left[ \begin{matrix} 15 & 0 & 0 & 0 \\ 0 & 50 & 20 & 25 \\ 35 & 5 & 0 & 10 \\ 0 & 65 & 50 & 65 \\ \end{matrix} \right] \to \left[ \begin{matrix} 15 & 0 & 0 & 0 \\ -5 & 45 & 15 & 20 \\ 30 & 0 & -5 & 5 \\ -5 & 60 & 45 & 60 \\ \end{matrix} \right]Now add 5 to each covered column: \left[ \begin{matrix} 15 & 0 & 0 & 0 \\ -5 & 45 & 15 & 20 \\ 30 & 0 & -5 & 5 \\ -5 & 60 & 45 & 60 \\ \end{matrix} \right] \to \left[ \begin{matrix} 20 & 0 & 5 & 0 \\ 0 & 45 & 20 & 20 \\ 35 & 0 & 0 & 5 \\ 0 & 60 & 50 & 60 \\ \end{matrix} \right]Now return to Step 3. Step 3: Cover all the zeros of the matrix with the minimum number of horizontal or vertical lines. \left[ \begin{matrix} 20 & 0 & 5 & 0 \\ 0 & 45 & 20 & 20 \\ 35 & 0 & 0 & 5 \\ 0 & 60 & 50 & 60 \\ \end{matrix} \right] \to \left[ \begin{matrix} \color{red}{20} & \color{red}0 & \color{red}5 & \color{red}0 \\ \color{red}0 & 45 & 20 & 20 \\ \color{red}{35} & \color{red}0 & \color{red}0 & \color{red}5 \\ \color{red}0 & 60 & 50 & 60 \\ \end{matrix} \right]Step 4: Since the minimal number of lines is still less than 4, we have to return to Step 5. Step 5: Note that 20 is the smallest entry not covered by a line. Subtract 20 from each uncovered row. \left[ \begin{matrix} 20 & 0 & 5 & 0 \\ 0 & 45 & 20 & 20 \\ 35 & 0 & 0 & 5 \\ 0 & 60 & 50 & 60 \\ \end{matrix} \right] \to \left[ \begin{matrix} 20 & 0 & 5 & 0 \\ -20 & 25 & 0 & 0 \\ 35 & 0 & 0 & 5 \\ -20 & 40 & 30 & 40 \\ \end{matrix} \right]Then add 20 to each covered column. \left[ \begin{matrix} 20 & 0 & 5 & 0 \\ -20 & 25 & 0 & 0 \\ 35 & 0 & 0 & 5 \\ -20 & 40 & 30 & 40 \\ \end{matrix} \right] \to \left[ \begin{matrix} 40 & 0 & 5 & 0 \\ 0 & 25 & 0 & 0 \\ 55 & 0 & 0 & 5 \\ 0 & 40 & 30 & 40 \\ \end{matrix} \right]Now return to Step 3. Step 3: Cover all the zeros of the matrix with the minimum number of horizontal or vertical lines. \left[ \begin{matrix} \color{red}{40} & \color{red}0 & \color{red}5 & \color{red}0 \\ \color{red}0 & \color{red}{25} & \color{red}0 & \color{red}0 \\ \color{red}{55} & \color{red}0 & \color{red}0 & \color{red}5 \\ \color{red}0 & \color{red}{40} & \color{red}{30} & \color{red}{40} \\ \end{matrix} \right]Step 4: Since the minimal number of lines is 4, an optimal assignment of zeros is possible and we are finished. \left[ \begin{matrix} 40 & 0 & 5 & \boxed0 \\ 0 & 25 & \boxed0 & 0 \\ 55 & \boxed0 & 0 & 5 \\ \boxed0 & 40 & 30 & 40 \\ \end{matrix} \right]Since the total cost for this assignment is 0, it must be an optimal assignment. Here is the same assignment made to the original cost matrix. Reference[1] Hungarian algorithm, Wikipedia [2] The Assignment Problem and the Hungarian Method, Bruff, Derek,]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[RANSAC]]></title>
    <url>%2F2019%2F07%2F03%2Falgorithm_ransac%2F</url>
    <content type="text"><![CDATA[Random sample consensusBrief introduction:An iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates. The algorithm was first published by Fischler and Bolles at SRI International in 1981. They used RANSAC to solve the Location Determination Problem (LDP), where the goal is to determine the points in the space that project onto an image into a set of landmarks with known locations. Algorithm:The RANSAC algorithm is essentially composed of two steps that are iteratively repeated: In the first step, a sample subset containing minimal data items is randomly selected from the input dataset. A fitting model and the corresponding model parameters are computed using only the elements of this sample subset. The cardinality of the sample subset is the smallest sufficient to determine the model parameters. In the second step, the algorithm checks which elements of the entire dataset are consistent with the model instantiated by the estimated model parameters obtained from the first step. A data element will be considered as an outlier if it does not fit the fitting model instantiated by the set of estimated model parameters within some error threshold that defines the maximum deviation attributable to the effect of noise. The set of inliers obtained for the fitting model is called consensus set. The RANSAC algorithm will iteratively repeat the above two steps until the obtained consensus set in certain iteration has enough inliers. Reference[1] RANSAC, Wikipedia]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Pinhole Camera Model]]></title>
    <url>%2F2019%2F07%2F01%2F3DV_PineholeCameraModel%2F</url>
    <content type="text"><![CDATA[Pinhole Camera ModelThe pinhole camera model describes the mathematical relationship between the coordinates of a point in three-dimensional space and its projection onto the image plane of an ideal pinhole camera, where the camera aperture is described as a point and no lenses are used to focus light. Model analysis \frac{-y_1}{f}=\frac{x_1}{x_3}\\ \frac{-y_2}{f}=\frac{x_2}{x_3}\\This can be summarized as: \left[ \begin{array}{ccc} y_1\\ y_2 \end{array} \right] = -\frac{f}{x_3} \left[ \begin{array}{ccc} x_1\\ x_2 \end{array} \right]Which is an expression that describes the relation between the 3D coordinates $(x_1,x_2,x_3)$ of point P and its image coordinates $(y_1,y_2)$ given by point Q in the image plane. Rotate the coordinate system in the image plane 180°, so the negation is removed: \left[ \begin{array}{ccc} y_1\\ y_2 \end{array} \right] = \frac{f}{x_3} \left[ \begin{array}{ccc} x_1\\ x_2 \end{array} \right]Camera matrix Homogeneous coordinates for equation above: \left[ \begin{array}{ccc} y_1\\ y_2\\ 1 \end{array} \right] = \frac{f}{x_3} \left[ \begin{array}{ccc} x_1\\ x_2\\ \frac{x_3}{f} \end{array} \right] = \frac{1}{x_3} \left[ \begin{array}{ccc} f & 0 & 0 & 0\\ 0 & f & 0 & 0\\ 0 & 0 & 1 & 0\\ \end{array} \right] \left[ \begin{array}{ccc} x_1\\ x_2\\ x_3\\ 1 \end{array} \right]Matrix $C$ is the camera matrix C = \left[ \begin{array}{ccc} f & 0 & 0 & 0\\ 0 & f & 0 & 0\\ 0 & 0 & 1 & 0 \end{array} \right] Intrinsic matrix Image coordinates to pixel coordinates: u = \frac{x}{dx} + u_0\\ v = \frac{y}{dy} + v_0That is: \left[ \begin{array}{ccc} u\\ v\\ 1 \end{array} \right] = \left[ \begin{array}{ccc} \frac{1}{dx} & 0 & u_0\\ 0 & \frac{1}{dy} & v_0\\ 0 & 0 & 1 \end{array} \right] \left[ \begin{array}{ccc} x\\ y\\ 1 \end{array} \right]Where dx and dy means the width of the pixel.(1 mm per pixel for example) Camera 3D coordinates to pixel coordinates: \left[ \begin{array}{ccc} y_1\\ y_2\\ 1 \end{array} \right] = \frac{1}{x_3} \left[ \begin{array}{ccc} \frac{1}{dx} & 0 & u_0\\ 0 &\frac{1}{dy} & v_0\\ 0 & 0 & 1 \end{array} \right] \left[ \begin{array}{ccc} f & 0 & 0 & 0\\ 0 & f & 0 & 0\\ 0 & 0 & 1 & 0\\ \end{array} \right] \left[ \begin{array}{ccc} x_1\\ x_2\\ x_3\\ 1 \end{array} \right] Combination of the two matrix: \left[ \begin{array}{ccc} y_1\\ y_2\\ 1 \end{array} \right] = \frac{1}{x_3} \left[ \begin{array}{ccc} \frac{f}{dx} & 0 & u_0 & 0\\ 0 & \frac{f}{dy} & v_0 & 0\\ 0 & 0 & 1 & 0\\ \end{array} \right] \left[ \begin{array}{ccc} x_1\\ x_2\\ x_3\\ 1 \end{array} \right]Matrix $I$ is the intrinsic matrix I = \left[ \begin{array}{ccc} \frac{f}{dx} & 0 & u_0 & 0\\ 0 & \frac{f}{dy} & v_0 & 0\\ 0 & 0 & 1 & 0\\ \end{array} \right] Extrinsic matrixWhich denote the coordinate system transformations from 3D world coordinates to 3D camera coordinates. Transformation: \left[ \begin{array}{ccc} x_c\\ y_c\\ z_c\\ 1 \end{array} \right] = \left[ \begin{array}{ccc} \textbf{R} & \textbf{T}\\ \textbf{0}^T & 1\\ \end{array} \right] \left[ \begin{array}{ccc} x_w\\ y_w\\ z_w\\ 1 \end{array} \right] R matrix: For a point $p$ in world coordinates, we can represent it as: p = x_w \cdot \vec{x} +y_w \cdot \vec{y} +z_w \cdot \vec{z}Where $\vec{x},\vec{y}$ and $\vec{z}$ are the unit vector of world coordinates. Now we only consider the rotation transformation between the two coordinates. For unit vectors in camera coordinates, we can represent them in world coordinates as: \vec{u} = r_{11} \cdot \vec{x} + r_{12} \cdot \vec{y} + r_{13} \cdot \vec{z}\\ \vec{v} = r_{21} \cdot \vec{x} + r_{22} \cdot \vec{y} + r_{23} \cdot \vec{z}\\ \vec{w} = r_{31} \cdot \vec{x} + r_{32} \cdot \vec{y} + r_{33} \cdot \vec{z}Where $\vec{u},\vec{v}$ and $\vec{w}$ are the unit vector of camera coordinates. And also we can rewrite the equation in matrix: \left[ \begin{array}{ccc} \vec{u}\\ \vec{v}\\ \vec{w} \end{array} \right] = \left[ \begin{array}{ccc} r_{11} & r_{12} & r_{13}\\ r_{21} & r_{22} & r_{23}\\ r_{31} & r_{32} & r_{23}\\ \end{array} \right] \left[ \begin{array}{ccc} \vec{x}\\ \vec{y}\\ \vec{z} \end{array} \right]So the point $p$ can be represented in camera coordinates as: p = x_w \cdot \vec{x} +y_w \cdot \vec{y} +z_w \cdot \vec{z}\\ = \left[ \begin{array}{ccc} x_w & y_w & z_w \end{array} \right] \left[ \begin{array}{ccc} \vec{x}\\ \vec{y}\\ \vec{z} \end{array} \right] = \left[ \begin{array}{ccc} x_w & y_w & z_w \end{array} \right] \left[ \begin{array}{ccc} r_{11} & r_{12} & r_{13}\\ r_{21} & r_{22} & r_{23}\\ r_{31} & r_{32} & r_{23}\\ \end{array} \right]^{-1} \left[ \begin{array}{ccc} \vec{u}\\ \vec{v}\\ \vec{w} \end{array} \right]And thus the relationship between $x_c, \, y_c, \, z_c$ and $x_w, \, y_w, \, z_w$ is: p = \left[ \begin{array}{ccc} x_c & y_c & z_c \end{array} \right] \left[ \begin{array}{ccc} \vec{u}\\ \vec{v}\\ \vec{w} \end{array} \right] = \left[ \begin{array}{ccc} x_w & y_w & z_w \end{array} \right] \left[ \begin{array}{ccc} r_{11} & r_{12} & r_{13}\\ r_{21} & r_{22} & r_{23}\\ r_{31} & r_{32} & r_{23}\\ \end{array} \right]^{-1} \left[ \begin{array}{ccc} \vec{u}\\ \vec{v}\\ \vec{w} \end{array} \right]We can get the equation as follows: \left[ \begin{array}{ccc} x_c\\ y_c\\ z_c \end{array} \right] = \Bigg( \left[ \begin{array}{ccc} r_{11} & r_{12} & r_{13}\\ r_{21} & r_{22} & r_{23}\\ r_{31} & r_{32} & r_{33} \end{array} \right]^{-1} \Bigg)^T \left[ \begin{array}{ccc} x_w\\ y_w\\ z_w \end{array} \right]Here the R matrix is just what we want: R = \Bigg( \left[ \begin{array}{ccc} r_{11} & r_{12} & r_{13}\\ r_{21} & r_{22} & r_{23}\\ r_{31} & r_{32} & r_{33} \end{array} \right]^{-1} \Bigg)^T T matrix: Then we can consider the translation. Of course it’s quite simple compared with the rotation situation. Assume that the position of the camera hole in the world coordinates is $(t_1, t_2, t_3)$, we have: \left[ \begin{array}{ccc} x_c\\ y_c\\ z_c \end{array} \right] = \left[ \begin{array}{ccc} x_w\\ y_w\\ z_w \end{array} \right] - \left[ \begin{array}{ccc} t_1\\ t_2\\ t_3 \end{array} \right]Here the T matrix is just what we want: T = \left[ \begin{array}{ccc} -t_1\\ -t_2\\ -t_3 \end{array} \right] Combination of rotation and translation: In homogeneous coordinates presentation, we can combine these two transformation together as: \left[ \begin{array}{ccc} x_c\\ y_c\\ z_c\\ 1 \end{array} \right] = \left[ \begin{array}{ccc} \textbf{R} & \textbf{T}\\ \textbf{0}^T & 1\\ \end{array} \right] \left[ \begin{array}{ccc} x_w\\ y_w\\ z_w\\ 1 \end{array} \right] Reference[1] Pinhole camera model, Wikipedia [2] Camera resectioning, Wikipedia]]></content>
      <categories>
        <category>三维视觉</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[检测模型评估之mAP]]></title>
    <url>%2F2019%2F06%2F04%2Fmeasure_mAP%2F</url>
    <content type="text"><![CDATA[The object detection problemGiven an image, find the objects in it, locate their position and classify them. Trained on a fixed set of classes, so the model would locate and classify only those classes in the image. The location of the object is generally in the form of a bounding rectangle. Dataset example: Actual image Notation Class X coordinate Y coordinate Box Width Box Height Dog 100 600 150 100 Horse 700 300 200 250 Person 400 400 100 500 Bounding box with class(only for visualization) The Mean Average Precision aka, the mAPEvery image in an object detection problem could have different objects of different classes. So both the classification and localization of a model need to be evaluated. IoUWe first need to know how to judge the correctness of each of those detections. The metric that tells us the correctness of a given bounding box is the IoU - Intersection over Union. It is a very simple visual quantity. The intersection includes the overlap area(the area colored in Cyan), and the union includes the Orange and Cyan regions both. The IoU will then be calculated like this Identifying correct detections and calculating precision and recallFor calculating Precision and Recall, as with all machine learning problems, we have to identify True Positives(TP), False Positives(FP), True Negatives(TN) and False Negatives(FN). TP and FP are indentified with IoU and a threshold in a specific class. If IoU &gt; threshold: True Positive(TP). If IoU &lt; threshold: False positive(FP). For calculating Recall, we need the count of Negatives. Since every part of the image where we didn’t predict an object is considered a negative, measuring “True” negatives is a bit futile. So we only measure False Negatives(FN) - ie. the objects that our model has missed out. Precision = \frac{TP}{(TP+FP)}\\ Recall = \frac{TP}{(TP+FN)}Calculating the Mean Average PrecisionUsing the Pascal VOC challenge evaluation metric The Mean Average Precision is a term which has different definitions. This metric is commonly used in the domains of Information Retrieval and Object Detection. Both these domains have different ways of calculating mAP. We will talk about the Object Detection relevant mAP today. The Object Detection definition of mAP was first formalized in the PASCAL Visual Objects Classes(VOC) challenge, which included various image processing tasks. For the exact paper refer to this. (backup) We use the same approaches for calculation of Precision and Recall as mentioned in the previous section. But, as mentioned, we have at least two other variables which determine the values of Precision and Recall, they are the IOU and the Confidence thresholds. The IOU is a simple geometric metric, which we can easily standardize, for example the PASCAL VOC challenge evaluates mAP based on 50% IOU, whereas the COCO Challenge goes a step further and evaluates mAP at various threshold ranging from 5% to 95%. The confidence factor on the other hand varies across models, 50% confidence in my model design might probably be equivalent to an 80% confidence in someone else’s model design, which would vary the precision recall curve shape. Hence the PASCAL VOC organizers came up with a way to account for this variation. We now need a metric to evaluate the models in a model agnostic way. The paper recommends that we calculate a measure called AP - ie. the Average Precision 123456789&gt; For a given task and class, the precision/recall curve is&gt; computed from a method’s ranked output. Recall is defined&gt; as the proportion of all positive examples ranked above a&gt; given rank. Precision is the proportion of all examples above&gt; that rank which are from the positive class. The AP summarises&gt; the shape of the precision/recall curve, and is de-&gt; fined as the mean precision at a set of eleven equally spaced&gt; recall levels [0,0.1,...,1]:&gt; AP = \frac{1}{11} \sum \limits_{r \in\{0, 0.1, 0.2, ... , 1\}}p_{interp}(r)This means that we chose 11 different confidence thresholds(which determine the “rank”). The thresholds should be such that the Recall at those confidence thresholds is 0, 0.1, 0.2, 0.3, … , 0.9 and 1.0. The AP is now defined as the mean of the Precision values at these chosen 11 Recall values. This results in the mAP being an overall view of the whole precision recall curve. The paper further gets into detail of calculating the Precision used in the above calculation. 1234&gt; The precision at each recall level r is interpolated by taking&gt; the maximum precision measured for a method for which&gt; the corresponding recall exceeds r:&gt; p_{interp}(r)= \max \limits_{\tilde r:\tilde r \ge r}p(\tilde r)\\ where\ p(\tilde r)\ is\ the\ measured\ precision\ at\ recall\ \tilde r.Basically we use the maximum precision for a given recall value. The mAP hence is the Mean of all the Average Precision values across all your classes as measured above. This is in essence how the Mean Average Precision is calculated for Object Detection evaluation. There might be some variation at times, for example the COCO evaluation is more strict, enforcing various metrics with various IOUs and object sizes(more details here). If any of you want me to go into details of that, do let me know in the comments. Some important points to remember when we compare mAP values mAP is always calculated over a dataset. Although it is not easy to interpret the absolute quantification of the model output, mAP helps us by being a pretty good relative metric. When we calculate this metric over popular public datasets, the metric can be easily used to compare old and new approaches to object detection. Depending on how the classes are distributed in the training data, the Average Precision values might vary from very high for some classes(which had good training data) to very low(for classes with less/bad data). So your mAP may be moderate, but your model might be really good for certain classes and really bad for certain classes. Hence it is advisable to have a look at individual class Average Precisions while analysing your model results. These values might also serve as an indicator to add more training samples. ReferenceMeasuring Object Detection models - mAP - What is Mean Average Precision?]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Generative Adversarial Nets]]></title>
    <url>%2F2019%2F05%2F29%2Fpaper_GAN%2F</url>
    <content type="text"><![CDATA[生成对抗网络同时训练两个网络, 分别是生成网络 $G​$ 和判别网络 $D​$. 其中 $G​$ 网络用于学习真实数据的分布, 而 $D​$ 网络用于判别输入的样本是来自真实数据还是来自 $G​$ 生成的伪数据. 研究背景:截至作者提出生成对抗网络之前, 深度学习的主要成就集中在判别网络的应用中. 这主要归功于反向传播和 $Dropout$ 算法的成功应用. 但是对于生成网络, 其损失函数的设计成了一个难题. 类似于极大似然估计这种损失函数难以很好地被应用在学习的过程当中. 生成网络:生成网络的任务就是学习一个数据分布 $p_g(\textbf{x})$ 来拟合真实数据的分布 $p_{data}(\textbf{x})$. 生成网络 $G$ 接受分布为 $p_{\textbf{z}}(\textbf{z})$ 的随机噪声 $\textbf{z}$ 作为输入, 则 $G$ 可视为从到噪声空间到数据空间的一个映射 ​$G(\textbf{z};\theta_g): \textbf{z} \to \textbf{x}$. 而函数 ​$G$ 是由多层感知机构成的, 以 ​$\theta_g$ 为参数的可导函数. 判别网络:判别网络与常见的判别式网络类似, 它完成一个二分类任务. 以数据样本作为输入, 然后输出一个标量用于表示输入的样本是来自真实数据的概率. 其形式化为 $D(\textbf{x};\theta_d)$. 损失函数:生成对抗网络理论上的损失函数如下: \min \limits_G \space \max \limits_D \space V(D,G) = E_{\textbf{x} \sim p_{data}(\textbf{x})}[logD(\textbf{x})] + E_{\textbf{z} \sim p_{\textbf{z}}(\textbf{z})}[log(1-D(G(\textbf{z})))]其直观解释是在最大化 $D​$ 对数据样本的判别能力的条件下，最小化 $D​$ 对 $G​$ 所生成样本的判别能力. 两个网络就像在进行一场博弈, 逐步提升各自的能力, 直到最后 $G​$ 学到的分布完全拟合真实数据分布, 从而达到一个最优的状态. 这时, $D​$ 的输出始终为 $1/2​$, 即无法对输入样本进行有效的判断. 训练过程:GAN 的训练过程是一个生成网络 $G​$ 和判别网络 $D​$ 交替迭代训练的过程, 即在训练 $D​$ 时固定 $G​$ 的参数, 而训练 $G​$ 时固定 $D​$ 的参数, 由此循环往复直到训练结束. 结合损失函数我们可以发现, 在训练 $G​$ 时,由于 $D​$ 被固定, $logD(\textbf{x})​$ 就成为了一个常数, 此时训练的目标就是最小化 $log(1-D(G(\textbf{z})))​$. 图1:生成对抗网络的训练算法 如上图所示的训练算法, 由训练样本的独立同分布采样假设, 理论损失函数中期望的计算可以简单地用样本的平均值来代替. 此外, 对于训练 $G$ 时所采用的 $log(1-D(G(\textbf{z})))$ 损失函数, 在早期会有梯度不足的问题. 因为早期的生成器 $G$ 的能力很弱, 生成的数据很容易被判别器 $D$ 分辨, 从而可能存在绝大多数的生成数据都被分辨出来而导致损失函数值接近 $0$ 的情况. 这时整个训练就进行地十分缓慢, 因为 $log$ 函数在函数值 $0$ 周围梯度比较小. 故而实际训练可以用最大化 $logD(G(\textbf{z}))$ 来代替原来的最小化 $log(1-D(G(\textbf{z})))​$ 的策略, 或者标签反转也是不错的选择. 图2:一个简单的例子 理论阐述:]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SiamFCN]]></title>
    <url>%2F2019%2F05%2F28%2Fpaper_Siam_FCN%2F</url>
    <content type="text"><![CDATA[利用孪生网络进行相似性学习训练一个 Siamese 网络, 利用相似性计算来实现在 search image 上定位 exemplar image. 训练图片:从被标注的视频(每帧中的目标都被 bounding box 框出)中获取用于训练的图片对, 两张图片的帧间隔小于T. 其中 search image 是以目标为中心的大小为 $255 \times 255$ 的图片, 超出原图的区域用合理的RGB值填充, 不对原图进行缩放. 而 exemplar image 是以目标为中心的大小为 $127 \times 127$ 的图片. 具体地, 若 bounding box(红色框) 的大小为 (w, h), padding 的大小为 p, 则调整因子 s 由以下约束决定, 个人理解下面的两个 s 是不同的调整因子, 最终得到的是一个正方形框图, 即对 $(w+2p) \times (h+2p)$ 以外而 $127 \times 127$ 以内的部分进行合理颜色填充. s(w + 2p) * s(h + 2p) = A\\ A = 127^2\\ p = (w + h) / 4\\ 图1:训练数据的获取 训练标注:由于训练帧的目标都在图片中心, 故而训练帧对应的 score map, 具体意义参见网络结构)的值由如下规则给出: 距离 score map 中心 R 以内的点为正例, 其他点为反例.(需考虑 stride 值, 其中 stride 值为 search image 中选出 candidate image 时的步长, 由 φ 网络中的池化层决定). 其中 u 为 score map D中的点, c 为 score map 的中心点, k为 stride 值. \textbf{y}[u]=\left\{ \begin{aligned} +1 & & if \space k||u - c|| \leq R \\ -1 & & otherwise \\ \end{aligned} \right.\\损失函数:单分数的损失函数和 score map 的平均损失函数定义如下, 其中 v 为前向传播得到的分数矩阵 score map, y 为真实标注的 score map. l(y,v) = log(1 + exp(-yv))\\ L(\textbf y,\textbf v) = \frac{1}{|D|} \sum_{u\in D} l(\textbf y[u],\textbf v[u])\\网络结构: φ 网络为全卷积网络,用于 exemplar image 和 candidate image 的特征提取, 图中的两个 φ 网络共享所有参数. exemplar image 经过 φ 网络得到其对应的大小为 $6\times 6\times 128$ 的特征映射. 由于 search image 的尺寸大于 exemplar image , 为了定位目标在 search image 中的位置,理论上需要需要穷举 search image 上所有可能的与 exemplar image 尺寸相同的候选子图. 而全卷积网络的计算共享的特性使得这样的穷举在一次前向传播中就能够实现. 全卷积网络作用在 search image 上的结果是得到一个大小为 $22\times 22\times 128$ 的特征映射,在这个特征映射中可以截取得到所有可能的 candidate iname 的特征映射. 将两张图片上得到的特征映射作互相关(内积)并作一定处理后得到 exemplar image 与 search image 上的各候选子图的相似度分数, 以 score map 的形式展现, 其中 score map 中的各个点的分数在几何上与 search image 中相应位置的 candidate image 一一对应. 图2: 孪生网络结构图 图3: 子网络φ超参数 训练过程:以一对训练图片为例, 将 exemplar image 喂给上图中的 z, 将 search image 喂给上图中的 x , 之后进行前向传播得到 score map, 根据训练标注中所定义的作为 ground truth 的分数矩阵计算 loss 执行反向传播算法更新全卷积网络 φ 中的参数. 跟踪测试: 利用第一帧中 bounding box 框出的待检测目标,通过与之前在训练集中使用的相同的裁剪填充操作得到exemplar image. 以前一帧检测到的目标位置作为中心,通过与之前在训练集中使用的相同的裁剪填充操作得到当前帧的 search image 并结合 exemplar image 执行前向传播算法得到对应 score map. 利用 score map 的最大值点相对于 score map 中心点的偏移乘以 stride 得到当前帧中目标相对于上一帧中目标的偏移量. 目标尺寸空间的跟踪通过对 search image 的不同缩放版本来实现. 考虑到目标移动的连续性, 引入 cosine window 来实现对大幅度偏移的惩罚, 目标尺寸的大幅度变化也会受到惩罚. 全卷积网络的特性: 输入大小可变 计算共享 如图 4 中, 当输入的大小大于 $16\times 16$ 时, 输出的大小相应变为 $2 \times 2$. 其中输出结果中的蓝色点对应了输入图片中蓝色部分的结果. 还可以看到, 由于网络中池化层的存在, 当 $14 \times 14$ 的 “滑动窗口” 移动 2 个像素时才会产生一个输出点. 这个移动距离就是文中提到的 stride 值, 论文 Overfeat 中将其比喻成 “滑动窗口的” 分辨率. 当然严格的计算共享需要保证网络中没有 padding 的存在. 图4: Overfeat中关于全卷积网络特性的解释]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[模型存取]]></title>
    <url>%2F2019%2F05%2F19%2Fpytorch_save_load_model%2F</url>
    <content type="text"><![CDATA[SAVING AND LOADING MODELS与众多的深度学习框架一样, Pytorch可以对训练好的模型进行存储, 将来需要时可以随时取用而不必花大量时间重新训练网络. 三个核心函数 torch.save torch.load torch.nn.Model.load_state_dict state_dict在 Pytorch 中, torch.nn.Module 的可学习参数可以由 model.parameters() 访问. state_dict 是一个 Python 的字典对象, 它将模型的层映射到各自的参数 Tensor. 12345678910111213141516171819202122232425262728293031323334353637383940import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optim# Define modelclass TheModelClass(nn.Module): def __init__(self): super(TheModelClass, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x# Initialize modelmodel = TheModelClass()# Initialize optimizeroptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)# Print model's state_dictprint("Model's state_dict:")for param_tensor in model.state_dict(): print(param_tensor, "\t", model.state_dict()[param_tensor].size())# Print optimizer's state_dictprint("Optimizer's state_dict:")for var_name in optimizer.state_dict(): print(var_name, "\t", optimizer.state_dict()[var_name]) 1234567891011121314Model&apos;s state_dict:conv1.weight torch.Size([6, 3, 5, 5])conv1.bias torch.Size([6])conv2.weight torch.Size([16, 6, 5, 5])conv2.bias torch.Size([16])fc1.weight torch.Size([120, 400])fc1.bias torch.Size([120])fc2.weight torch.Size([84, 120])fc2.bias torch.Size([84])fc3.weight torch.Size([10, 84])fc3.bias torch.Size([10])Optimizer&apos;s state_dict:state &#123;&#125;param_groups [&#123;&apos;lr&apos;: 0.001, &apos;momentum&apos;: 0.9, &apos;dampening&apos;: 0, &apos;weight_decay&apos;: 0, &apos;nesterov&apos;: False, &apos;params&apos;: [140040066551024, 140041977181024, 140041977181096, 140040066553608, 140040066552384, 140040066319272, 140040066319344, 140040066319416, 140040066319488, 140040066319560]&#125;] state_dict的保存与加载注意必须在加载 state_dict 之后调用 model.eval() 来设置模型的 dropout 和 batch normalization 层. 否则会报错. 1234567# 保存torch.save(model.state_dict(), PATH)# 加载model = TheModelClass(*args, **kwargs)model.load_state_dict(torch.load(PATH))model.eval() 整个模型的保存与加载缺点是只能保存给定的类和路径的模型, 因为这种保存方法不保存模型的类本身, 相反它保存一个包含模型的类的定义的文件路径, 故而在加载模型之前, 该模型的类必须事先存在定义, 且路径必须与保存时相同. 1234567# 保存torch.save(model, PATH)# 加载# Model class must be defined somewheremodel = torch.load(PATH)model.eval() 如果没有在与保存模型时相同的相对路径下, 加载模型时会出现如下的错误 12345678&gt;&gt;&gt; model = torch.load('/home/myy/Desktop/haha.pth')Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "/home/myy/.conda/envs/pytorch/lib/python3.7/site-packages/torch/serialization.py", line 387, in load return _load(f, map_location, pickle_module, **pickle_load_args) File "/home/myy/.conda/envs/pytorch/lib/python3.7/site-packages/torch/serialization.py", line 574, in _load result = unpickler.load()AttributeError: Can't get attribute 'TheModelClass' on &lt;module '__main__' (built-in)&gt; 断点的保存与加载保存断点以便继续训练或者什么的 12345678910111213141516171819202122# 保存torch.save(&#123; 'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'loss': loss, ... &#125;, PATH)# 加载 model = TheModelClass(*args, **kwargs)optimizer = TheOptimizerClass(*args, **kwargs)checkpoint = torch.load(PATH)model.load_state_dict(checkpoint['model_state_dict'])optimizer.load_state_dict(checkpoint['optimizer_state_dict'])epoch = checkpoint['epoch']loss = checkpoint['loss']model.eval()# - or -model.train() 参考文献: [1]Pytorch - Docs [2]Pytorch - Tutorials: saving and loading modules]]></content>
      <categories>
        <category>Pytorch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[利用GPU训练]]></title>
    <url>%2F2019%2F05%2F17%2Fpytorch_train_with_gpu%2F</url>
    <content type="text"><![CDATA[Train with GPU之前我们已经讨论过 Pytorch 对与 CUDA 的良好支持, 那么自然地我们可以使用强悍的 GPU 设备来训练搭建好的神经网络, 以此来达到加速训练的效果. 毕竟炼丹还是需要良好的硬件支持的是吧. 单GPU训练使用单个 GPU 训练的方法非常简单, 我们只需要将网络部署到 CUDA 的 GPU 设备上并且将数据也部署到相同的设备上即可. 部署的方式与我们之前讨论过的 Tensor 的部署方法相同. 123&gt;&gt;&gt; device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")&gt;&gt;&gt; net.to(device)&gt;&gt;&gt; inputs, labels = inputs.to(device), labels.to(device) 注意此处 tensor.to(device) 仅仅将原 Tensor 在 GPU 上作一个拷贝并返回拷贝的 Tensor, 并不对原 Tensor 有任何修改. 多GPU训练默认情况下 Pytorch 只会使用单个 GPU 进行模型的训练, 你可以使用 DataParallel 来实现多 GPU 的模型计算, 其原型为 1torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0) 参数解释如下: module: 需要多 GPU 并行计算的模型 device_ids: CUDA 设备的 ID, 默认为全部可用设备 output_device: 输出所在设备, 默认为 device_ids[0] 12345if torch.cuda.device_count() == 3: print("Let's use", torch.cuda.device_count(), "GPUs!") model = nn.DataParallel(model, device_ids=[0, 1, 2])model.to(device) 没错就是这么简单, 结束了. 参考文献: [1]Pytorch - Docs [2]Pytorch - Tutorials: A 60 Minute Blitz]]></content>
      <categories>
        <category>Pytorch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[神经网络搭建]]></title>
    <url>%2F2019%2F05%2F17%2Fpytorch_nn%2F</url>
    <content type="text"><![CDATA[torch.nn我们可以利用 torch.nn 来搭建神经网络, 其中 nn.Mudule 你可以定义一系列网络层以及一个 forward(input) 方法从输入获取神经网络的输出. 神经网络训练流程 定义带有可训练参数 (权重) 的神经网络结构 训练数据输入神经网络前向传播获得输出 利用数据标签和网络输出计算损失函数 执行反向传播算法, 获得损失函数的输出相对于各参数的梯度值 利用梯度值更新网络的权值 定义网络在模型中只需到定义 forward() 方法, 而 backward() 方法会自动定义. 在 forward() 方法中可以使用任何的对 Tensor 的操作. 下面的代码是 LeNet 的 Pytorch 实现: 1234567891011121314151617181920212223242526272829303132333435import torchimport torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): super(Net, self).__init__() # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square you can only specify a single number x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_features 123456789&gt;&gt;&gt; net = Net()&gt;&gt;&gt; print(net)Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True)) 模型中可以学习的参数可以由 net.parameters() 返回, 参数是一个个的 Tensor, requires_grad 标志位为 True. 12345&gt;&gt;&gt; params = list(net.parameters())&gt;&gt;&gt; print(len(params))10&gt;&gt;&gt; print(params[0].size()) # conv1's .weighttorch.Size([6, 1, 5, 5]) 前向传播我们可以尝试给如上的网络一个随机的输入, 观察他的输出情况. 注意此处的输入 size 为 (1, 1, 32, 32), 这是因为 torch.nn 只支持 mini-batches 的输入. 例如, 对于 nn.Conv2d , 它所要求的输入是一个 4 维的 Tensor, 即: nSamples * nChannels * Height * Width. 故而如果你的输入是单个样本, 可以利用 input.unsqueeze(0) 来添加一个虚拟维度. 12345&gt;&gt;&gt; input = torch.randn(1, 1, 32, 32)&gt;&gt;&gt; out = net(input)&gt;&gt;&gt; print(out)tensor([[-0.1258, -0.0585, -0.0102, -0.0407, -0.0695, -0.0808, 0.0731, -0.1482, 0.0575, -0.0164]], grad_fn=&lt;AddmmBackward&gt;) 损失函数损失函数使用 (output, target) 对作为输出, 计算出的值用于度量 output 相对于 target 的距离. porch.nn 中定义了许多现成的损失函数. 一个最简单的例子便是均方误差函数 nn.MSELoss . 123456output = net(input)target = torch.randn(10) # a dummy target, for exampletarget = target.view(1, -1) # make it the same shape as outputcriterion = nn.MSELoss()loss = criterion(output, target) 12&gt;&gt;&gt; print(loss)tensor(0.5244, grad_fn=&lt;MseLossBackward&gt;) 反向传播由于多次反向传播的梯度值的累加机制, 我们在进行一次反向传播更新权重之前, 我们需要用 zero_grad() 对所有参数的 grad 清零, 然后对 loss 调用 backward() 方法就可以得到当前损失函数的输出关于各可训练参数的梯度了. 12345678&gt;&gt;&gt; net.zero_grad()&gt;&gt;&gt; print(net.conv1.bias.grad)tensor([0., 0., 0., 0., 0., 0.])&gt;&gt;&gt; loss.backward()&gt;&gt;&gt; print(net.conv1.bias.grad)tensor([ 0.0185, 0.0110, 0.0038, 0.0098, -0.0105, -0.0002]) 更新网络权重一个最简单的权重更新方法是随机梯度下降(SGD), 他的策略是 weight = weight - learning_rate * gradient , 我们可以简单实现如下: 123learning_rate = 0.01for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) 不过, 作为一个成熟的深度学习框架, Pytorch 应该学会自己更新权重, 故而 torch.optim 实现了许多权重更新的算法供我们使用, 而且使用方法非常简单: 12345678910111213import torch.optim as optim# create your optimizeroptimizer = optim.SGD(net.parameters(), lr=0.01)# in your training loop:optimizer.zero_grad() # zero the gradient buffersoutput = net(input)loss = criterion(output, target)loss.backward()optimizer.step() # Does the update 参考文献: [1]Pytorch - Docs [2]Pytorch - Tutorials: A 60 Minute Blitz]]></content>
      <categories>
        <category>Pytorch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[张量的创建]]></title>
    <url>%2F2019%2F05%2F15%2Fpytorch_tensor_create%2F</url>
    <content type="text"><![CDATA[torch.TensorTensor 是 Pytorch 的数据结构, 非常类似于 Numpy 里的 ndarray. 是一种包含单一数据类型的多维张量. 其上定义了诸多的操作. 如我们讨论过的自动求导机制等. 张量的创建torch.tensor() 1torch.tensor(data,dtype=None,device=None,requires_grad=False,pin_memory=False) 利用上述函数创建 Tensor 时, 它总是拷贝 data 内的数据然后利用这些数据结合相应参数构造一个新的叶结点变量, 故而 torch.tensor(x) 与 x.clone().detach() 等价, 而 torch.tensor(x,requires_grad=True) 则与 x.clone().detach().rquires_grad_(True) 等价. 如果想要避免上述的拷贝-创建机制, 请不要使用这个函数 :-). 而如果你想得到一个与原变量共享内存的新 Tensor, 那么可以使用 torch.Tensor.detach() 或者 torch.as_tensor() 等方法. 前者可以从原来的 Tensor 创造一个 requires_grad 标志位为 False 且不在原 Tensor 计算图中的 Tensor, 两者共享内存. 而后者可从 Numpy 的一个 ndarray 创造 Tensor , 两者共享内存. 如果你想修改原 Tensor 的性质, 可以使用 torch.Tensor.requires_grad_() 或者 torch.Tensor.detach_(). 前者使 Tensor 的 requires_grad 标志位为真, 自动求导机制开始记录操作.后者与其不含下划线的版本不同之处在于它修改原 Tensor 的性质, 即将原 Tensor 从计算图中剥离并且置 requires_grad 为 False. 示例代码: 12345&gt;&gt;&gt; import torch&gt;&gt;&gt; a = [1., 1.]&gt;&gt;&gt; b = [2., 2.]&gt;&gt;&gt; x = torch.tensor(a, requires_grad = True)&gt;&gt;&gt; y = torch.tensor(a, requires_grad = False) 123456&gt;&gt;&gt; x[0] = -1&gt;&gt;&gt; xtensor([-1., 1.], grad_fn=&lt;CopySlices&gt;)&gt;&gt;&gt; a[1.0, 1.0]# 可见 x 与 a 并不共享内存 1234567891011&gt;&gt;&gt; x_ = x.detach()&gt;&gt;&gt; xtensor([-1., 1.], grad_fn=&lt;CopySlices&gt;)&gt;&gt;&gt; x_tensor([-1., 1.])&gt;&gt;&gt; x[0] = 1&gt;&gt;&gt; xtensor([1., 1.], grad_fn=&lt;CopySlices&gt;)&gt;&gt;&gt; x_tensor([1., 1.])# 可见 x 与 x_ 共享内存 torch.as_tensor() 1torch.as_tensor(data, dtype=None, device=None) 如果 data 是一个 Tensor 且数据类型与设备参数相同, 则直接共享该 Tensor 的内存给新 Tensor. 如果 data 是一个 Tensor 而数据类型或设备不符, 则会新创建一个 Tensor, 新 Tensor 保持原 Tensor 的计算图和 requires_grad 标志位. 如果 data 是一个数据类型相同的 ndarray , 且参数中指定的设备为 CPU, 则新 Tensor 与该 ndarray 共享内存. 示例代码: 1234567891011&gt;&gt;&gt; a = numpy.array([1, 2, 3])&gt;&gt;&gt; t = torch.as_tensor(a)&gt;&gt;&gt; t[0] = -1&gt;&gt;&gt; aarray([-1, 2, 3])&gt;&gt;&gt; a = numpy.array([1, 2, 3])&gt;&gt;&gt; t = torch.as_tensor(a, device=torch.device('cuda'))&gt;&gt;&gt; t[0] = -1&gt;&gt;&gt; aarray([1, 2, 3]) 其他一些创建方法123456789101112131415161718192021&gt;&gt;&gt; torch.ones(5, 6)tensor([[1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.]])&gt;&gt;&gt; q = torch.zeros(5, 6)tensor([[0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0.]])&gt;&gt;&gt; torch.ones_like(q)tensor([[1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1.]])...... 参考文献: [1]Pytorch - Docs [2]Pytorch - Tutorials: A 60 Minute Blitz]]></content>
      <categories>
        <category>Pytorch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[自动求导机制]]></title>
    <url>%2F2019%2F05%2F14%2Fpytorch_autograd%2F</url>
    <content type="text"><![CDATA[自动求导requires_grad 标志位Pytorch 允许我们利用每个 Tensor 的 requires_grad 标志位来在计算图中排除一些不需要进行梯度计算的子图. 一个操作的输入中只要有一个Tensor的 requires_grad 为True, 则该操作的输出的 requires_grad 为True.相反只有输入中所有的Tensor的 requires_grad 为False时该操作的输出的 requires_grad 才为False. 123456789&gt;&gt;&gt; x = torch.randn(5, 5) # requires_grad = False by default&gt;&gt;&gt; y = torch.randn(5, 5) # requires_grad = False by default&gt;&gt;&gt; z = torch.randn((5, 5), requires_grad = True)&gt;&gt;&gt; a = x + y&gt;&gt;&gt; a.requires_gradFalse&gt;&gt;&gt; b = a + z&gt;&gt;&gt; b.requires_gradTrue 在 model 中我们可以通过如下的操作改变其参数的 requires_grad 标志位以实现特定的需求, 如通过设置标志位为False来进行参数微调、跑测试集等. 123model = torchvision.models.resnet18(pretrained = True)for param in model.parameters(): param.requires_grad = False; 当然我们也可以用如下的方法禁用梯度计算 123456&gt;&gt;&gt; import torch&gt;&gt;&gt; x = torch.randn((5, 5), requires_grad = True)&gt;&gt;&gt; with torch.no_grad():... y = x + 1&gt;&gt;&gt; y.requires_gradFalse 反向传播求导如果你想要求输出 Tensor 关于各参数的偏导数, 可对输出的 Tensor 调用 backward() 方法 . 如果输出的 Tensor 是一个标量, 则无需对 backward() 指定任何参数. 而如果输出的 Tensor 不是一个标量, 则需为该方法指定一个参数, 该参数为一个 Tensor, 其形状与输出的 Tensor 相同, 其值作为计算梯度时输出 Tensor 各元素乘上的系数. 输出为标量的例子: 1234567891011&gt;&gt;&gt; x = torch.ones(2, 2, requires_grad = True)&gt;&gt;&gt; y = x + 2&gt;&gt;&gt; z = y * y * 3&gt;&gt;&gt; out = z.mean()&gt;&gt;&gt; outtensor(27., grad_fn=&lt;MeanBackward0&gt;)&gt;&gt;&gt; out.backward()&gt;&gt;&gt; x.grad #输出为out对x各元素的偏导数tensor([[4.5000, 4.5000], [4.5000, 4.5000]]) 输出为向量的例子: 1234567&gt;&gt;&gt; x = torch.tensor([[1., 1.], [2., 2.]], requires_grad = True)&gt;&gt;&gt; y = torch.tensor([[3., 3.], [4., 4.]], requires_grad = True) &gt;&gt;&gt; z = x.mm(y)&gt;&gt;&gt; z.backward(torch.ones(2, 2))&gt;&gt;&gt; x.gradtensor([[6., 8.], [6., 8.]]) 可以发现当输出不是标量时, 如果传给 backward() 的参数为一个全一的 Tensor (正如我们上面所做), 则反向传播求得的对输入的梯度值是输出 Tensor 中各个元素对输入的梯度的累加值. 若想单独得到输出 Tensor 的某一个元素对输入的梯度值, 则可以通过给 backward() 提供特定的参数实现, 比如下面的例子就展示了矩阵乘法情况下输出Tensor z 的左上角元素对输入的梯度求解过程. 12345678&gt;&gt;&gt; x = torch.tensor([[1., 1.], [2., 2.]], requires_grad = True)&gt;&gt;&gt; y = torch.tensor([[3., 3.], [4., 4.]], requires_grad = True) &gt;&gt;&gt; z = x.mm(y)&gt;&gt;&gt; a = torch.tensor([[1., 0.], [0., 0.]])&gt;&gt;&gt; z.backward(a)&gt;&gt;&gt; x.gradtensor([[3., 4.], [0., 0.]]) 如果调用 backward() 方法时没有设置参数 retain_graph = True, 则在完成一次 backward() 后计算图会被释放, 无法进行第二次反向传播. 设置 retain_graph = True 的作用是保持计算图不释放, 每次反向传播的梯度累加. 而若需要将 Tensor 的梯度清零, 则可调用 zero_() 函数: 123456789&gt;&gt;&gt; x.gradtensor([[6., 8.], [6., 8.]])&gt;&gt;&gt; x.grad.zero_()tensor([[0., 0.], [0., 0.]])&gt;&gt;&gt; x.gradtensor([[0., 0.], [0., 0.]]) 参考文献: [1]Pytorch - Docs [2]Pytorch - Tutorials: A 60 Minute Blitz]]></content>
      <categories>
        <category>Pytorch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CUDA支持]]></title>
    <url>%2F2019%2F05%2F14%2Fpytorch_cuda%2F</url>
    <content type="text"><![CDATA[torch.cudaPytorch 提供了 CUDA 的支持, torch.cuda 模块用于设置和运行 CUDA 上的操作, 它会记住所指定的 GPU, 你后续所新建的 CUDA 上的 Tensor 都将被默认在该 GPU 上创建. 当然你也可以用torch.cuda.device修改 CUDA 默认指定的 GPU 或者为 Tensor 单独指定 GPU. 模块不支持跨 GPU 的操作, 除非使用类似拷贝的函数功能将变量转换到同一设备上, 如to()和cuda()等． 为cuda上的Tensor指定设备 为每个Tensor单独指定/转换到一个设备 123456789101112131415161718&gt;&gt;&gt; import torch&gt;&gt;&gt; cuda = torch.device('cuda') # 使用cuda默认的设备GPU0&gt;&gt;&gt; cuda1 = torch.device('cuda:1') # 指定GPU1&gt;&gt;&gt; cuda2 = torch.device('cuda:2') # 指定GPU2&gt;&gt;&gt; x = torch.tensor([1., 2.], device = cuda)&gt;&gt;&gt; x.devicedevice(type='cuda', index=0)&gt;&gt;&gt; y = torch.tensor([1., 2.])&gt;&gt;&gt; y.devicedevice(type='cpu')&gt;&gt;&gt; y = y.cuda(cuda1)&gt;&gt;&gt; y.devicedevice(type='cuda', index=1)&gt;&gt;&gt; z = torch.tensor([1., 2.])device(type='cpu')&gt;&gt;&gt; z = z.to(cuda2)&gt;&gt;&gt; z.devicedevice(type='cuda', index=2) 设置上下文默认设备 123456789101112131415161718# 指定cuda的GPU为GPU1&gt;&gt;&gt; with torch.cuda.device(1):... a = torch.tensor([1., 2.], device = cuda)... b = torch.tensor([1., 2.]).cuda()... c = torch.tensor([1., 2.]).to(device = cuda)... d = a + b + c... # 指定上下文后还是可以为每个tensor单独指定设备... s = torch.tensor([1., 2.]).to(device = cuda2)&gt;&gt;&gt; a.devicedevice(type='cuda', index=1)&gt;&gt;&gt; b.devicedevice(type='cuda', index=1)&gt;&gt;&gt; c.devicedevice(type='cuda', index=1)&gt;&gt;&gt; d.devicedevice(type='cuda', index=1)&gt;&gt;&gt; s.devicedevice(type='cuda', index=2) 1234print("Outside device is 0") # On device 0 (default in most scenarios)with torch.cuda.device(1): print("Inside device is 1") # On device 1print("Outside device is still 0") # On device 0 Pytorch中CUDA使用的一个范例由参数和主机的 CUDA 是否可用共同决定是使用 CPU 还是 CUDA 123456789101112import argparseimport torchparser = argparse.ArgumentParser(description='PyTorch Example')parser.add_argument('--disable-cuda', action='store_true', help='Disable CUDA')args = parser.parse_args()args.device = Noneif not args.disable_cuda and torch.cuda.is_available(): args.device = torch.device('cuda')else: args.device = torch.device('cpu') 有了制定了设备后就可以进行 Tensor 或者 model 的部署了 12x = torch.empty((8, 42), device=args.device)net = Network().to(device=args.device) 参考文献: [1]Pytorch - Docs [2]Pytorch - Tutorials: A 60 Minute Blitz]]></content>
      <categories>
        <category>Pytorch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[似然函数作为损失函数]]></title>
    <url>%2F2019%2F05%2F13%2Fml_loss_function%2F</url>
    <content type="text"><![CDATA[似然函数在数理统计学中, 似然函数是一种关于统计模型中的参数的函数, 表示模型参数的似然性. 统计学中似然性和概率有着明确的区分. 概率用于在已知一些参数的情况下, 预测接下来在观测上所得到的结果. 而似然性则是用于在已知某些观测所得到的结果时, 对相关参数进行估值. 似然函数的重要性不是它的具体取值, 而是当参数变化时函数到底是变大还是变小. 对同一个似然函数, 其所代表的模型中, 某项参数值具有多种可能, 但如果存在一个参数值使得它的函数值达到最大的话, 那么这个值就是该项参数最为”合理”的参数值. 在已知某个参数 $B$ 时, 事件 $A$ 会发生的概率写作: P(A|B) = \frac{P(A,B)}{P(B)}由贝叶斯公式, 可知: P(B|A) = \frac{P(A|B)P(B)}{P(A)}故而, 我们可以反过来构造表示似然性的方法, 即已知有事件 $A$ 发生, 运用似然函数 $L(B|A)$ 估计参数 $B$ 的可能性.前面我们已经提到, 似然函数的重要性在于相对大小而不要求满足归一化条件. 因此我们将一个似然函数乘以一个正的常数后仍然是似然函数. 而对于特定的问题, $\frac{P(B)}{P(A)}$ 为不变的常数, 则: L(b|A) = \alpha P(A|B=b)当令 $\alpha=1$ 时, 关于参数 $B$ 的似然函数与给定参数 $B$ 的值后观测 $A$ 的条件概率等价. 似然函数作为损失函数既然似然函数可以用于在已知观测结果时对模型参数进行估值, 那么自然地可以作为机器学习中的损失函数来指导训练过程. 考虑判别式模型中的场景, 作为损失函数的似然函数接收 $\textbf{label}$ 与 $\textbf{predict}$ 两部分输入. 其中 $\textbf{label}$ 为训练集的标签也就是一次观测时各类别的确切概率值, 如 $[0,…0,1,0…0]$. 而 $\textbf{predict}$ 为模型在给定输入后输出的各类别预测概率分布. 似然函数的目的就是衡量 $\textbf{predict}$ 背后的模型对于观测值,即 $\textbf{label}​$ 的解释程度. 那么此时结合之前似然函数的讨论, 我们需要最大化似然函数, 即使得基于预测 $\textbf{predict}$ 时产生观测 $\textbf{label}$ 的概率最大. 单个样本时, 我们计算给定 $\textbf{predict}$ 时产生观测 $\textbf{label}$ 的概率如下: P = \prod_{i=1}^{C}predict(i)^{label(i)}\\ 注:其中C为类别数目下面我们对整个样本集(符合独立同分布定理)进行计算分析, 可将似然函数等价为条件概率: L = \prod_{j=1}^{m}\prod_{i=1}^{C}predict(i)^{label(i)}\\ 注:其中m为训练集大小由于指数以及累乘的存在, 很自然的我们想到对似然函数进行对数运算. 于是得到了所谓的对数似然函数. 由于对数的单调特性, 最大化似然函数等价于最大化对数似然函数. L = \sum\limits_{j=1}^{m}\sum\limits_{i=1}^{C}label(i)*log(predict(i))\\对数似然函数与交叉熵对数似然函数的形式与交叉熵极其相似, 事实上两者只相差了常系数项. 将上式中的对数似然函数除以m并取负数便得到所谓的负对数似然函数, 与交叉熵的形式一模一样. 于是原优化问题转化为最小化负对数似然函数, 即交叉熵. 参考文献: [1]维基百科-似然函数 [2]机器之心-似然函数与交叉熵]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习之核函数]]></title>
    <url>%2F2019%2F05%2F12%2Fml_kernel_method%2F</url>
    <content type="text"><![CDATA[核函数对于线性不可分样本问题(如简单的异或问题), 可以将样本从原始空间映射到一个更高维的特征空间, 使得样本在这个特征空间内线性可分. 例如在支持向量机中, 原问题转化为求解特征空间上的凸二次规划问题: f(\textbf{x}) = w^T \phi(\textbf{x}) + b对上述线性可分支持向量机的求解涉及到如下对偶问题的求解(由拉格朗日乘子法得到对偶问题的具体过程此处不予讨论): \mathop{max}_\alpha \quad \sum\limits_{i = 1}^{n}\alpha_i-\frac{1}{2}\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{m}\alpha_i\alpha_jy_iy_j\phi(\textbf{x}_i)^T\phi(\textbf{x}_j)\\ s.t. \quad \sum\limits_{i=1}^{m}\alpha_iy_i=0\\ \alpha_i \ge 0, \quad i=1,2,...,m求解上式涉及到计算 $\phi(\textbf{x}_i)^T\phi(\textbf{x}_j)$, 这是样本 $\textbf{x}_i$ 与$\textbf{} $$\textbf{x}_j$ 映射到特征空间之后的内积. 由于特征空间维数很高, 甚至可能是无穷维, 因此直接计算通常是困难的. 为了避开这个障碍, 可以设想这样一个函数: k(\textbf{x}_i,\textbf{x}_j)==\phi(\textbf{x}_i)^T\phi(\textbf{x}_j)即 $\textbf{x}_i$ 与 $\textbf{x}_j$ 在特征空间的内积等于它们在原始样本空间中通过函数 $k(.,.)$ 计算的结果(此即核技巧). 有了这样的函数我们就不必直接去计算高维甚至无穷维特征空间中的内积, 于是对偶问题重写并求解可得 f(\textbf{x})=w^T\phi(\textbf{x})+b\\ =\sum\limits_{i=1}^{m}\alpha_iy_i\phi(\textbf{x}_i)^T\phi(\textbf{x})+b\\ =\sum\limits_{i=1}^{m}\alpha_iy_ik(\textbf{x},\textbf{x}_i)+b\\这里的函数 $k(.,.)$ 就是 “核函数” 显然, 如果已知合适的映射 $\phi(.)$ 的具体形式, 则可写出核函数 $k(.,.)$. 但是 现实任务重我们通常不知道 $\phi(.)$ 是什么形式. 剩下的问题就是如何选择 $\phi(.)$: 使用一个通用的 $\phi$, 例如无限维的 $\phi$, 它隐含地用在高斯核也即基于径向基函数 $(RBF)$ 核的核机器上 手动地设计 $\phi$ , 在深度学习出现以前, 这一直是主流的方法 深度学习的策略是去学习 $\phi$ 参考文献: [1]机器学习-周志华-第126, 127页 [2]深度学习-花书-第106页]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Inline Assembler in GCC]]></title>
    <url>%2F2018%2F10%2F05%2Finline-asm-gcc%2F</url>
    <content type="text"><![CDATA[操作系统上机知识储备(一)对gcc所支持的内联汇编格式及使用方法进行简要的介绍 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117/* * This c file is a introduction of inline Assembly for gcc * Time： 2018/9/8 * Author: myy * Reference： JeffreyLi https://www.jianshu.com/p/1782e14a0766*//////////////////////////////////////////////////////////////////////////////////////////////*Formular: asm ( assembler template : output operands // optional : input operands // optional : list of clobbered registers // optional ); The assembler template form: * form1:"sentence?sentence?sentence" * form2: "sentence?" "sentence?" "sentence" --- Where the ? should be replaced by "\n\t" or ";". --- Access to C variables by %0 %1 and so on. The operands form: * "constraint" (C expression) --- Where constraint is mainly used for note Addressing type(memory or register). --- For output operants, it should be decorated by "=", which means write-only. --- For multiple operants, they should be seperated by ",". --- To use the operants in assempler template with %0 %1 %2 ... - First output operants then input operants. - Signed in the list order. --- The output operants must be lvalue. The clobber list: * When a register is used in the assembler template, it should be in the clobber list. Which let gcc no longer assume that the values previously stored in these registers are still legal. * If the instruction changes the memory value in an unpredictable form, you need to add "memory" to the clobbered list.This allows GCC not to cache these memory values. The volatile: * If the assembly code is required to be executed at the location where it was placed (for example, it cannot be moved out of the loop during the optimization), we need to put a "volatile" keyword before the "()" after asm. This will prevent these codes from being moved or deleted. --- Useage: __asm__ __volatile__(...); Common constraints: *********************** * r Register(s) * a %eax, %ax, %al * b %ebx, %bx, %bl * c %ecx, %cx, %cl * d %edx, %dx, %adl * S %esi, %si * D %edi, %di *********************** * m memory *********************** --- If use the register constraint, the value will first be stored at the register used, and then write back to the memory in the end. --- Constraint "0" means: use the same register as the first output operants. Constraint Modifiers: * "=": Write only. * "&amp;": Means that this operand is an earlyclobber operand, which is modified before the instruction is finished using the input operands. Therefore, this operand may not lie in a register that is used as an input operand or as a part of any memory address.An input operand can be tied to an earlyclobber operand if its only use as an input occurs before the early result is written.*/////////////////////////////////////////////////////////////////////////////////////////////#include&lt;stdio.h&gt;int main()&#123; int a = 10,b = 5,c; //Example: to realize c = a + b __asm__ __volatile__ ( "movl %2,%%eax\n\t movl %1,%%ebx\n\t addl %%ebx,%%eax\n\t movl %%eax,%0" :"=r"(c) :"r"(a),"r"(b) :"eax","ebx" ); printf("c = a + b = %d\n",c); return 0;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[FFT with python]]></title>
    <url>%2F2018%2F04%2F01%2Ffft-by-python%2F</url>
    <content type="text"><![CDATA[来自信号与系统的一个小作业用python和其开源的科学计算库numpy实现快速傅里叶变换算法，并用数据可视化开源第三方库matplotlib作出频域图样。 代码实现如下12345678910111213141516171819202122232425262728293031323334#导入第三方库###############################from numpy import *from math import *import matplotlib.pyplot as plt################################ TO GET THE DATA（take gauss signal sampling for example）data_list = [ exp(-pi*x*x/1024/1024) for x in range(0,1024)]# FFT FUNCTION PART (递归蝶形算法，输入为数据点数目和数据点列)def my_fft_fun(NUM, DATA): if NUM == 1: return DATA else: ODD_DATA=DATA[0::2] EVEN_DATA=DATA[1::2] ODD_LIST=my_fft_fun(NUM//2, ODD_DATA) #奇偶分治 EVEN_LIST=my_fft_fun(NUM//2, EVEN_DATA) #奇偶分治 RES_DATA=[ EVEN_LIST[k%(NUM//2)]+ODD_LIST[k%(NUM//2)]*(cos(2*pi*k/NUM)+sin(2*pi*k/NUM)*1j) for k in range(0,NUM)] return RES_DATA#调用定义好的FFT递归求解函数FINAL_RES=my_fft_fun(1024,data_list)# 作出X(k)的模值与k的关系图x_part=range(0,1024)y_part=[ abs(val) for val in FINAL_RES]plt.scatter(x_part,y_part,s=5)plt.title('DFT for Gauss Signal', fontsize=20)plt.xlabel('k', fontsize=14)plt.ylabel('X(k)', fontsize=14)plt.show() 效果作图如下 有待改进之处算法仅涉及简单情形，即数据点的个数是2的指数倍，当需要计算非2的指数倍个数据点的FFT时，分治算法需要更加复杂。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Zero contact]]></title>
    <url>%2F2018%2F02%2F08%2Fzero-contact%2F</url>
    <content type="text"><![CDATA[不如来段紧张刺激的Hello,World代码如何： 1&gt;&gt;&gt; print("Hello,World!") THE END…]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F01%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
